---
title: "Grouping and quality checks"
author : "Jeremy Wicquart"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: "cosmo"
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
options(knitr.duplicate.label = "allow")

```

# Load packages

```{r base}

# 1. Source functions ----

source("00_functions/graphical_par.R")
source("00_functions/theme_graph.R")
source("00_functions/render_rmd.R")

# 2. Required packages ----

library(tidyverse) # Core tidyverse packages
library(formattable) # Interactive HTML tables
library(DT) # Interactive HTML tables
library(leaflet) # Interactive HTML map
library(sf)
sf_use_s2(FALSE)
library(rmarkdown)
library(plotly)
library(kableExtra)
require(prettydoc)

# 3. Set theme_graph() as the default ggplot theme ----

theme_set(theme_graph())

```

# Data grouping

```{r warning=FALSE}

# 1. Get list of csv file ----

files_list <- list.files(path = "./../data/02_standardized-data", full.names = TRUE)

# 2. Bind all files together ----

synthetic_data <- map_dfr(files_list, ~read_csv(., show_col_types = FALSE))

# 3. Check incorrect variable names ----

setdiff(colnames(synthetic_data), c(var_names, "organismID"))

# 4. Remove useless data sets and variables ----

rm(files_list)

```

# Taxonomical re-categorisation

```{r}

# 1. Misc. corrections on organismID levels ----

synthetic_data <- synthetic_data %>% 
  # Homogenize organismID
  mutate(organismID = str_to_lower(organismID), # To lowercase
         organismID = gsub("[()]", "", organismID), # Remove parentheses
         organismID = iconv(organismID, from = 'UTF-8', to = 'ASCII//TRANSLIT'), # Convert accent letters
         organismID = str_replace_all(organismID, c("&" = "and",
                                                    "_" = " ",
                                                    "," = " ",
                                                    "\\." = " ")), # Convert some special characters
         organismID = str_squish(organismID)) # Remove duplicated space and space before and after

# 2. Add new organismID levels to the tax-recategorisation file ----

current_unique_organismID <- synthetic_data %>% select(organismID) %>% distinct() %>% pull() # vector

old_unique_organismID <- read.csv2("./../data/03_tax-recategorisation.csv") %>% 
  filter(organismID %in% current_unique_organismID) # Remove organismID that have been deleted
  
current_unique_organismID <- synthetic_data %>% select(organismID) %>% distinct() # tibble
  
anti_join(current_unique_organismID, old_unique_organismID, by = c("organismID")) %>% 
  mutate(checked = FALSE) %>% 
  bind_rows(old_unique_organismID, .) %>% 
  arrange(organismID) %>%  
  write.csv2(., file = "./../data/03_tax-recategorisation.csv", row.names = FALSE)

rm(current_unique_organismID, old_unique_organismID)

# ---------------------------------------------------------------------------------------------- #
# /!\          Before to run the next chunk code, fill the exported csv file:                /!\ #
# /!\          Manually complete empty tax. variables for each unique organismID             /!\ #
# /!\  The text of the column "organismID" must not be changed: it's the grouping variable   /!\ #
# ---------------------------------------------------------------------------------------------- #

```

# Join taxonomy

```{r}

# 1. Add "category" and "subcategory" ----

unique_organismID <- read.csv("./../data/03_tax-recategorisation.csv", sep = ";") %>% 
  # Remove eventual white spaces
  mutate(across(c("category", "subcategory", "condition", "phylum",
                  "class", "subclass", "order", "family", "genus", "species"),
                ~str_squish(.))) %>% 
  # Assign "subcategory"
  mutate(subcategory = case_when(phylum == "Cyanobacteria" ~ "Cyanobacteria",
                                 order == "Corallinales" ~ "Coralline algae",
                                 class %in% c("Phaeophyceae", "Florideophyceae", "Ulvophyceae") & order != "Corallinales" ~ "Macroalgae",
                                 TRUE ~ subcategory)) %>% 
  # Assign "category"
  mutate(category = case_when(order == "Scleractinia" ~ "Hard coral",
                              phylum %in% c("Porifera", "Chordata", "Echinodermata", "Bryozoa") ~ "Other fauna",
                              phylum == "Cyanobacteria" ~ "Algae",
                              class %in% c("Ascidiacea", "Hydrozoa", "Crinoidea", 
                                           "Bivalvia", "Echinoidea", "Anthozoa") ~ "Other fauna",
                              order %in% c("Actiniaria", "Alcyonacea", "Zoantharia", 
                                           "Corallimorpharia", "Antipatharia") ~ "Other fauna",
                              subclass %in% c("Octocorallia") ~ "Other fauna",
                              phylum %in% c("Annelida", "Mollusca", "Arthropoda") ~ "Other fauna",
                              family %in% c("Milleporidae", "Helioporidae") ~ "Hard coral",
                              class %in% c("Ulvophyceae", "Florideophyceae", "Phaeophyceae") ~ "Algae",
                              phylum %in% c("Chlorophyta", "Rhodophyta", "Ochrophyta", "Foraminifera") ~ "Algae",
                              phylum == "Tracheophyta" ~ "Seagrass",
                              TRUE ~ category)) %>% 
  select(-checked)

# 2. Join main data and recategorized organismID ----

synthetic_data <- left_join(synthetic_data, unique_organismID, by = "organismID")

# 3. Check if 'genus' is correct (i.e. if it correspond to the first word of 'species') ----

genus_check <- unique_organismID %>%
  filter(!is.na(species))

if(all(str_split_fixed(genus_check$species, " ", 2)[,1] == genus_check$genus, na.rm = TRUE) == FALSE){
  stop("All genus names contained in the 'Species' variable are NOT identical to those in the 'Genus' variable")
}

# 4. Control the filled categories ----

synthetic_data %>% 
  drop_na(category) %>% 
  select(organismID, category, subcategory, condition, phylum, 
         class, subclass, order, family, genus, species) %>% 
  distinct(.) %>% 
  formattable(.) %>% 
  as.datatable(., rownames = FALSE)

# 5. Control the unfilled categories (which will be removed) ----

synthetic_data %>% 
  filter(is.na(category)) %>% 
  select(organismID, category, subcategory, condition, phylum,
         class, subclass, order, family, genus, species) %>% 
  distinct(.) %>% 
  formattable(.) %>% 
  as.datatable(., rownames = FALSE)

# 6. Remove useless data sets and variables ----

rm(genus_check, unique_organismID)

```

```{r}

# 7. Make the sum of percentage cover for identical levels ----

synthetic_data <- synthetic_data %>% 
  drop_na(category, measurementValue) %>% 
  group_by(across(c(-measurementValue))) %>% 
  summarise(measurementValue = sum(measurementValue)) %>% 
  ungroup()

```

# Spatial assignation

```{r}

# 1. Extract site coordinates and transform to sf format ----

synthetic_data_coords <- synthetic_data %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(datasetID, decimalLatitude, decimalLongitude) %>% 
  distinct() %>% 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326)

```

## GCRMN spatial layers

```{r}

# 1. Load data ----

load("../data/gcrmn_regions.RData")
load("../data/gcrmn_subregions.RData")
load("../data/gcrmn_ecoregions.RData")

# 2. Make the spatial join ----

synthetic_data <- st_join(synthetic_data_coords, data_gcrmn_regions) %>% 
  st_join(., data_gcrmn_subregions %>% select(-region)) %>% 
  st_join(., data_gcrmn_ecoregions) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 3. Remove useless data sets and variables ----

rm(data_gcrmn_region, data_gcrmn_subregions, data_gcrmn_ecoregions)

```

## EEZ

```{r}

# 1. First assignation ----

# 1.1 Load EEZ data --

data_eez <- st_read("../data/07_data-eez/02_clean/eez_v12.shp")

# 1.2 Make the spatial join --

synthetic_data <- st_join(synthetic_data_coords, data_eez) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, 
         country = SOVEREIGN1, territory = TERRITORY1) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 2. Second assignation ----

# 2.1 Load EEZ buffer data (1 km) --

data_eez_buffer <- st_read("../data/07_data-eez/03_buffer/eez_v12_buffer.shp")

# 2.2 Extract sites with missing assignations (country and territory) --

synthetic_data_coords_buffer <- synthetic_data %>% 
  filter(is.na(country) | is.na(territory)) %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(datasetID, decimalLatitude, decimalLongitude) %>% 
  distinct() %>% 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326)

# 2.3 Make the spatial join --

synthetic_data_buffer <- st_join(synthetic_data_coords_buffer, data_eez_buffer) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, 
         country = SOVEREIGN1, territory = TERRITORY1) %>% 
  st_drop_geometry() %>% 
  group_by(decimalLongitude, decimalLatitude) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  mutate(country = if_else(n == 1, country, NA_character_),
         territory = if_else(n == 1, territory, NA_character_)) %>% 
  distinct() %>% 
  select(-n)

# 2.4 Join with other data --

synthetic_data_buffer <- synthetic_data %>% 
  filter(is.na(country) | is.na(territory)) %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(-country, -territory) %>% 
  left_join(synthetic_data_buffer, .)

# 3. Group data from first and second assignation --

synthetic_data <- synthetic_data %>% 
  filter(!(is.na(country) | is.na(territory)) | is.na(decimalLatitude) | is.na(decimalLongitude)) %>% 
  bind_rows(., synthetic_data_buffer)

# 4. Remove useless data sets and variables ----

rm(data_eez, data_eez_buffer, synthetic_data_coords_buffer, synthetic_data_buffer)

```

# Re-order variables

```{r}

synthetic_data <- synthetic_data %>% 
  rename(scientificName = species) %>% 
  select(datasetID, region, subregion, ecoregion, country, territory, locality, habitat, parentEventID,
         eventID, decimalLatitude, decimalLongitude, verbatimDepth, year, month, day, 
         eventDate, samplingProtocol, recordedBy, category, subcategory, condition, 
         phylum, class, order, family, genus, scientificName, measurementValue)

```

# Quality checks

```{r}

# 1. Quality checks number 1, 2, 3, 5, 6, 7, and 9 ----

synthetic_data <- synthetic_data %>% 
  mutate(qc1 = if_else(!(is.na(decimalLatitude)) | !(is.na(decimalLongitude)), TRUE, FALSE),
         qc2 = if_else(decimalLatitude >= -90 & decimalLatitude <= 90, TRUE, FALSE),
         qc3 = if_else(decimalLongitude >= -180 & decimalLongitude <= 180, TRUE, FALSE),
         qc5 = if_else(!(is.na(region)), TRUE, FALSE),
         qc6 = if_else(is.na(territory), FALSE, TRUE),
         qc7 = if_else(!(is.na(year)), TRUE, FALSE),
         qc9 = if_else(measurementValue >= 0 & measurementValue <= 100, TRUE, FALSE))

# 2. Quality check number 4 ----

reef_buffer <- st_read("../data/08_quality-checks-buffer/reefs-buffer_gee/reef_buffer.shp") %>% 
  st_transform(crs = 4326) %>% 
  st_wrap_dateline() %>% 
  st_make_valid()

synthetic_data <- st_intersects(synthetic_data_coords, reef_buffer, sparse = FALSE) %>% 
  as_tibble() %>% 
  rename(qc4 = 1) %>% 
  bind_cols(synthetic_data_coords, .) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 4. Quality check number 8 (a) ----

synthetic_data <- synthetic_data %>% 
  group_by(datasetID, locality, habitat, parentEventID,
           eventID, decimalLatitude, decimalLongitude, verbatimDepth, 
           year, month, day, eventDate) %>% 
  summarise(qc8_a = round(sum(measurementValue), 5)) %>% 
  ungroup() %>% 
  mutate(qc8_a = if_else(qc8_a >= 0 & qc8_a <= 100, TRUE, FALSE)) %>% 
  left_join(synthetic_data, .)

# 5. Quality check number 8 (b) ----

## 5.1 Adjust measurementValue ----
# To avoid deleting sampling unit whose total perc. cover is between 100 % and 101 %

synthetic_data <- synthetic_data %>%
  group_by(datasetID, locality, habitat, parentEventID,
         eventID, decimalLatitude, decimalLongitude, verbatimDepth, 
         year, month, day, eventDate) %>% 
  mutate(total = sum(measurementValue)) %>% 
  ungroup() %>% 
  mutate(measurementValue = ifelse(total > 100 & total <= 101, # Threshold = 101
                                   (measurementValue*100)/total, 
                                   measurementValue)) %>% 
  select(-total)

## 5.2 Quality check ----

synthetic_data <- synthetic_data %>% 
  group_by(datasetID, locality, habitat, parentEventID,
           eventID, decimalLatitude, decimalLongitude, verbatimDepth, 
           year, month, day, eventDate) %>% 
  summarise(qc8_b = round(sum(measurementValue), 5)) %>% 
  ungroup() %>% 
  mutate(qc8_b = if_else(qc8_b >= 0 & qc8_b <= 100, TRUE, FALSE)) %>% 
  left_join(synthetic_data, .)

# 5. Calculate percentage of rows removed because of each QC ----

## 5.1 For all quality checks except QC 8 (a) ----

quality_checks <- synthetic_data %>% 
  mutate(across(c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8_a", "qc8_b", "qc9"), 
                ~str_replace_all(as.character(.), 
                                 c("FALSE" = cur_column(), 
                                   "TRUE" = NA_character_)))) %>% 
  mutate(qc = coalesce(qc1, qc2, qc3, qc4, qc5, qc6, qc7, qc8_b, qc9)) %>% 
  group_by(datasetID, qc) %>% 
  count() %>% 
  ungroup() %>% 
  bind_rows(., tibble(datasetID = NA,
                      qc = c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8_b", "qc9"),
                      n = NA)) %>% 
  complete(datasetID, qc, fill = list(n = 0)) %>% 
  drop_na(datasetID) %>% 
  group_by(datasetID) %>% 
  mutate(percent = round(n*100/sum(n), 2)) %>% 
  ungroup()

## 5.2 For quality checks QC 8 (b) ----

quality_checks <- synthetic_data %>% 
  mutate(across(c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8_a", "qc8_b", "qc9"), 
                ~str_replace_all(as.character(.), 
                                 c("FALSE" = cur_column(), 
                                   "TRUE" = NA_character_)))) %>% 
  mutate(qc = coalesce(qc1, qc2, qc3, qc4, qc5, qc6, qc7, qc8_a, qc9)) %>% 
  group_by(datasetID, qc) %>% 
  count() %>% 
  ungroup() %>% 
  bind_rows(., tibble(datasetID = NA,
                      qc = c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8_a", "qc9"),
                      n = NA)) %>% 
  complete(datasetID, qc, fill = list(n = 0)) %>% 
  drop_na(datasetID) %>% 
  group_by(datasetID) %>% 
  mutate(percent = round(n*100/sum(n), 2)) %>% 
  ungroup() %>% 
  filter(qc == "qc8_a") %>% 
  bind_rows(quality_checks, .)

# 6. Remove useless data sets and variables ----

rm(reef_buffer, synthetic_data_coords)

```

# Individual summary

```{r}

map(unique(synthetic_data$datasetID), ~render_rmd(.))

```

# Remove rows based on quality check values

```{r}

synthetic_data <- synthetic_data %>%
  filter_at(c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8_b", "qc9"), ~.x == TRUE) %>% 
  select(-starts_with("qc"))

```

# Export data

```{r}

# 1. All data ----

save(synthetic_data, file = "../data/09_gcrmndb_benthos.RData")

# 2. Site coordinates ----

synthetic_data %>% 
  select(datasetID, decimalLatitude, decimalLongitude, year) %>% 
  distinct() %>% 
  group_by(datasetID, decimalLatitude, decimalLongitude) %>% 
  summarise(interval_years = max(year, na.rm = TRUE) - min(year, na.rm = TRUE)) %>%
  ungroup() %>% 
  mutate(int_class = cut(interval_years, 
                              breaks = c(-Inf, 1, 5, 10, 15, Inf),
                              labels = c("1 year", "2-5 years", "6-10 years", "11-15 years", ">15 years")),
         int_class = as.factor(int_class)) %>% 
  select(-interval_years) %>% 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326) %>% 
  # Use getwd to get the directory because gdal use a different relative path
  st_write(.,
           dsn = paste0(str_remove(getwd(), "/code"),
                        "/data/11_site-coords/gcrmndb-benthos_site-coords.shp"),
           append = FALSE)

```

---
Jeremy WICQUART | jeremywicquart@gmail.com | `r format(Sys.time())`