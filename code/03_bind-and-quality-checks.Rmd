---
title: "Grouping and quality checks"
author : "Jeremy Wicquart"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: "cosmo"
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Load packages

```{r base}

# 1. Source functions ----

source("../.Rprofile") # Rprofile with NCBI API key for taxize
source("00_functions/graphical_par.R")
source("00_functions/theme_graph.R")
source("00_functions/render_rmd.R")

# 2. Required packages ----

library(tidyverse) # Core tidyverse packages
library(formattable) # Interactive HTML tables
library(DT) # Interactive HTML tables
library(leaflet) # Interactive HTML map
library(taxize) # For taxonomy
library(sf)
sf_use_s2(FALSE)
library(rmarkdown)
library(plotly)
library(kableExtra)

# 3. Set theme_graph() as the default ggplot theme ----

theme_set(theme_graph())

```

# Data grouping

```{r warning=FALSE}

# 1. Get list of csv file ----

files_list <- list.files(path = "./../data/02_standardized-data", full.names = TRUE)

# 2. Bind all files together ----

synthetic_data <- map_dfr(files_list, ~read_csv(.))

# 3. Check incorrect variable names ----

setdiff(colnames(synthetic_data), c(var_names, "organismID"))

# 4. Remove useless data sets and variables ----

rm(files_list)

```

# Taxonomical re-categorisation

```{r}

# 1. Misc. corrections on organismID levels ----

synthetic_data <- synthetic_data %>% 
  # Homogenize organismID
  mutate(organismID = str_to_lower(organismID), # To lowercase
         organismID = gsub("[()]", "", organismID), # Remove parentheses
         organismID = iconv(organismID, from = 'UTF-8', to = 'ASCII//TRANSLIT'), # Convert accent letters
         organismID = str_replace_all(organismID, c("&" = "and",
                                                    "_" = " ",
                                                    "," = " ",
                                                    "\\." = " ")), # Convert some special characters
         organismID = str_squish(organismID)) # Remove duplicated space and space before and after

```

```{r}

# 1. Find upper tax. levels and manually complete those not found ----

# 1.A If older file exist --

if(file.exists("./../data/03_tax-recategorisation.csv")){
  
  # 1.A.1 Import it -
  
  current_unique_organismID <- synthetic_data %>% select(organismID) %>% distinct() %>% pull() # vector

  old_unique_organismID <- read.csv2("./../data/03_tax-recategorisation.csv") %>% 
    filter(organismID %in% current_unique_organismID) # Remove organismID that have been deleted
  
  current_unique_organismID <- synthetic_data %>% select(organismID) %>% distinct() # tibble
  
  # 1.A.2 Find rows absent in the older file (old_unique_organismID) but present 
  # in the current one (current_unique_organismID) and complete it -
  
  missing_organismID <- anti_join(current_unique_organismID, old_unique_organismID, by = c("organismID")) %>% 
    pull(.) %>% 
    tax_name(sci = ., 
             get = c("phylum", "class", "subclass", "order", "family", "genus", "species"), 
             db = "ncbi",
             messages = FALSE,
             ask = FALSE) # Disable choice if "more than one UID found"

  if(nrow(missing_organismID) > 0){
    
      missing_organismID %>%
        select(-db) %>% 
        rename(organismID = query) %>% 
        mutate(category = NA, subcategory = NA, condition = NA, .after = "organismID") %>% 
        mutate(checked = FALSE) %>% 
        bind_rows(old_unique_organismID, .) %>%
        arrange(organismID) %>%  
        write.csv2(., file = "./../data/03_tax-recategorisation.csv", row.names = FALSE)
    
  }
  
  rm(current_unique_organismID, old_unique_organismID, missing_organismID)

}else{
  
  # 1.B Else export it --
  
  synthetic_data %>% 
    select(organismID) %>% 
    distinct() %>% 
    pull(.) %>% 
    tax_name(sci = ., 
             get = c("phylum", "class", "subclass", "order", "family", "genus", "species"), 
             db = "ncbi",
             messages = FALSE, 
             ask = FALSE) %>% # Disable choice if "more than one UID found"
    select(-db) %>% 
    rename(organismID = query) %>% 
    mutate(category = NA, subcategory = NA, condition = NA, .after = "organismID") %>% 
    mutate(checked = FALSE) %>% 
    arrange(organismID) %>% 
    write.csv2(., file = "./../data/03_tax-recategorisation.csv", row.names = FALSE)
  
}

# ---------------------------------------------------------------------------------------------- #
# /!\          Before to run the next chunk code, fill the exported csv file:                /!\ #
# /!\          Manually complete empty tax. variables for each unique organismID             /!\ #
# /!\  The text of the column "organismID" must not be changed: it's the grouping variable   /!\ #
# ---------------------------------------------------------------------------------------------- #

```

# Join taxonomy

```{r}

# 1. Add "category" and "subcategory" ----

unique_organismID <- read.csv2("./../data/03_tax-recategorisation.csv") %>% 
  # Remove eventual white spaces
  mutate_at(c("category", "subcategory", "condition", "phylum", "class", "subclass", "order", 
              "family", "genus", "species"), ~str_squish(str_trim(., side = "both"))) %>% 
  # Assign "subcategory"
  mutate(subcategory = case_when(phylum == "Cyanobacteria" ~ "Cyanobacteria",
                                 order == "Corallinales" ~ "Coralline algae",
                                 TRUE ~ subcategory)) %>% 
  # Assign "category"
  mutate(category = case_when(order == "Scleractinia" ~ "Hard coral",
                              phylum %in% c("Porifera", "Chordata", "Echinodermata", "Bryozoa") ~ "Other fauna",
                              phylum == "Cyanobacteria" ~ "Algae",
                              class %in% c("Ascidiacea", "Hydrozoa", "Crinoidea", 
                                           "Bivalvia", "Echinoidea", "Anthozoa") ~ "Other fauna",
                              order %in% c("Actiniaria", "Alcyonacea", "Zoantharia", 
                                           "Corallimorpharia", "Antipatharia") ~ "Other fauna",
                              subclass %in% c("Octocorallia") ~ "Other fauna",
                              phylum %in% c("Annelida", "Mollusca") ~ "Other fauna",
                              family %in% c("Milleporidae", "Helioporidae") ~ "Hard coral",
                              class %in% c("Ulvophyceae", "Florideophyceae", "Phaeophyceae") ~ "Algae",
                              phylum %in% c("Chlorophyta", "Rhodophyta", "Ochrophyta") ~ "Algae",
                              TRUE ~ category)) %>% 
  select(-checked)

# 2. Join main data and recategorized organismID ----

synthetic_data <- left_join(synthetic_data, unique_organismID, by = "organismID")

# 3. Check if 'genus' is correct (i.e. if it correspond to the first word of 'species') ----

genus_check <- unique_organismID %>%
  filter(!is.na(species))

if(all(str_split_fixed(genus_check$species, " ", 2)[,1] == genus_check$genus, na.rm = TRUE) == FALSE){
  stop("All genus names contained in the 'Species' variable are NOT identical to those in the 'Genus' variable")
}

# 4. Control the filled categories ----

synthetic_data %>% 
  drop_na(category) %>% 
  select(organismID, category, subcategory, condition, phylum, 
         class, subclass, order, family, genus, species) %>% 
  distinct(.) %>% 
  formattable(.) %>% 
  as.datatable(., rownames = FALSE)

# 5. Control the unfilled categories (which will be removed) ----

synthetic_data %>% 
  filter(is.na(category)) %>% 
  select(organismID, category, subcategory, condition, phylum,
         class, subclass, order, family, genus, species) %>% 
  distinct(.) %>% 
  formattable(.) %>% 
  as.datatable(., rownames = FALSE)

# 6. Remove useless data sets and variables ----

rm(genus_check, unique_organismID)

```

```{r}

# 7. Make the sum of percentage cover for identical levels ----

synthetic_data <- synthetic_data %>% 
  drop_na(category, measurementValue) %>% 
  filter(measurementValue != 0) %>% 
  group_by(across(c(-measurementValue))) %>% 
  summarise(measurementValue = sum(measurementValue)) %>% 
  ungroup()

```

# Spatial assignation

```{r}

# 1. Extract site coordinates and transform to sf format ----

synthetic_data_coords <- synthetic_data %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(datasetID, decimalLatitude, decimalLongitude) %>% 
  distinct() %>% 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326)

```

## GCRMN region

```{r}

# 1. Load data ----

load("../data/06_gcrmn-regions.RData")

# 2. Make the spatial join ----

synthetic_data <- st_join(synthetic_data_coords, data_gcrmn_regions) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, higherGeography = gcrmn_region) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 3. Remove useless data sets and variables ----

rm(data_gcrmn_regions)

```

## EEZ

```{r}

# 1. First assignation ----

# 1.1 Load EEZ data --

data_eez <- st_read("../data/07_data-eez/eez_v11.shp") %>% 
  select(SOVEREIGN1, TERRITORY1) %>% 
  st_transform(crs = 4326) %>% 
  st_make_valid()

# 1.2 Make the spatial join --

synthetic_data <- st_join(synthetic_data_coords, data_eez) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, 
         country = SOVEREIGN1, territory = TERRITORY1) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 2. Second assignation ----

# 2.1 Load EEZ buffer data (5 km) --

data_eez_buffer <- st_read("../data/07_data-eez/eez_v11_buffer_1km.shp") %>% 
  select(SOVEREIGN1, TERRITORY1)

# 2.2 Extract sites with missing assignations (country and territory) --

synthetic_data_coords_buffer <- synthetic_data %>% 
  filter(is.na(country) | is.na(territory)) %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(datasetID, decimalLatitude, decimalLongitude) %>% 
  distinct() %>% 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326)

# 2.3 Make the spatial join --

synthetic_data_buffer <- st_join(synthetic_data_coords_buffer, data_eez_buffer) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, 
         country = SOVEREIGN1, territory = TERRITORY1) %>% 
  st_drop_geometry() %>% 
  group_by(decimalLongitude, decimalLatitude) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  mutate(country = if_else(n == 1, country, NA_character_),
         territory = if_else(n == 1, territory, NA_character_)) %>% 
  distinct() %>% 
  select(-n)

# 2.4 Join with other data --

synthetic_data_buffer <- synthetic_data %>% 
  filter(is.na(country) | is.na(territory)) %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(-country, -territory) %>% 
  left_join(synthetic_data_buffer, .)

# 3. Group data from first and second assignation --

synthetic_data <- synthetic_data %>% 
  filter(!(is.na(country) | is.na(territory)) | is.na(decimalLatitude) | is.na(decimalLongitude)) %>% 
  bind_rows(., synthetic_data_buffer)

# 4. Remove useless data sets and variables ----

rm(data_eez, data_eez_buffer, synthetic_data_coords_buffer, synthetic_data_buffer)

```

# Re-order variables

```{r}

synthetic_data <- synthetic_data %>% 
  rename(scientificName = species) %>% 
  select(datasetID, higherGeography, country, territory, locality, habitat, parentEventID,
         eventID, decimalLatitude, decimalLongitude, verbatimDepth, year, month, day, 
         eventDate, samplingProtocol, recordedBy, category, subcategory, condition, 
         phylum, class, order, family, genus, scientificName, measurementValue)

```

# Quality checks

```{r}

# 1. Quality checks number 1, 2, 3, 5, 6, and 8 ----

synthetic_data <- synthetic_data %>% 
  mutate(qc1 = if_else(!(is.na(decimalLatitude)) | !(is.na(decimalLongitude)), TRUE, FALSE),
         qc2 = if_else(decimalLatitude >= -90 & decimalLatitude <= 90, TRUE, FALSE),
         qc3 = if_else(decimalLongitude >= -180 & decimalLongitude <= 180, TRUE, FALSE),
         qc5 = if_else(is.na(territory), FALSE, TRUE),
         qc6 = if_else(!(is.na(year)), TRUE, FALSE),
         qc8 = if_else(measurementValue >= 0 & measurementValue <= 100, TRUE, FALSE))

# 2. Quality check number 4 ----

reef_buffer <- st_read("../data/08_quality-checks-buffer/reefs-buffer_gee/reef_buffer.shp") %>% 
  st_transform(crs = 4326) %>% 
  st_wrap_dateline() %>% 
  st_make_valid()

synthetic_data <- st_intersects(synthetic_data_coords, reef_buffer, sparse = FALSE) %>% 
  as_tibble() %>% 
  rename(qc4 = 1) %>% 
  bind_cols(synthetic_data_coords, .) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 4. Quality check number 7 ----

synthetic_data <- synthetic_data %>% 
  group_by(datasetID, locality, habitat, parentEventID,
           eventID, decimalLatitude, decimalLongitude, verbatimDepth, 
           year, month, day, eventDate) %>% 
  summarise(qc7 = sum(measurementValue)) %>% 
  ungroup() %>% 
  mutate(qc7 = if_else(qc7 >= 0 & qc7 <= 100, TRUE, FALSE)) %>% 
  left_join(synthetic_data, .)

# 5. Transform data ----

quality_checks <- synthetic_data %>% 
  mutate(across(c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8"), 
                ~as.character(.))) %>% 
  mutate(across(c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8"), 
                ~str_replace_all(., 
                                 c("FALSE" = cur_column(), 
                                   "TRUE" = NA_character_)))) %>% 
  mutate(qc = coalesce(qc1, qc2, qc3, qc4, qc5, qc6, qc7, qc8)) %>% 
  group_by(datasetID, qc) %>% 
  count() %>% 
  ungroup() %>% 
  bind_rows(., tibble(datasetID = NA, qc = c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8"), n = NA)) %>% 
  complete(datasetID, qc, fill = list(n = 0)) %>% 
  drop_na(datasetID) %>% 
  group_by(datasetID) %>% 
  mutate(percent = round(n*100/sum(n), 2)) %>% 
  ungroup()

# 6. Remove useless data sets and variables ----

rm(reef_buffer, synthetic_data_coords)

```

# Individual summary

```{r}

map(unique(synthetic_data$datasetID), ~render_rmd(.))

```

# Adjust measurementValue

```{r}

synthetic_data <- synthetic_data %>%
  group_by(datasetID, locality, habitat, parentEventID,
         eventID, decimalLatitude, decimalLongitude, verbatimDepth, 
         year, month, day, eventDate) %>% 
  mutate(total = sum(measurementValue)) %>% 
  ungroup() %>% 
  mutate(measurementValue = ifelse(total > 100 & total < 101, # Threshold = 101
                                   (measurementValue*100)/total, 
                                   measurementValue)) %>% 
  select(-total)

```

# Remove rows based on quality check values

```{r}

synthetic_data <- synthetic_data %>%
  filter_at(c("qc1", "qc2", "qc3", "qc4", "qc5", "qc6", "qc7", "qc8"), ~.x == TRUE) %>% 
  select(-starts_with("qc"))

```

# Export data

```{r}

save(synthetic_data, file = "../data/09_gcrmndb_benthos.RData")

```

---
Jeremy WICQUART | jeremywicquart@gmail.com | `r format(Sys.time())`