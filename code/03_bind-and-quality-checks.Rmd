---
title: "Grouping and quality checks"
author : "Jeremy Wicquart"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: "cosmo"
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Load packages

```{r base}

# 1. Source functions ----

source("../.Rprofile") # Rprofile with NCBI API key for taxize
source("00_functions/graphical_par.R")
source("00_functions/theme_graph.R")
source("00_functions/render_rmd.R")

# 2. Required packages ----

library(tidyverse) # Core tidyverse packages
library(formattable) # Interactive HTML tables
library(DT) # Interactive HTML tables
library(leaflet) # Interactive HTML map
library(taxize) # For taxonomy
library(sf)
sf_use_s2(FALSE)
library(rmarkdown)
library(plotly)
library(kableExtra)

# 3. Set theme_graph() as the default ggplot theme ----

theme_set(theme_graph())

```

# Data grouping

```{r warning=FALSE}

# 1. Get list of csv file ----

files_list <- list.files(path = "./../data/02_standardized-data", full.names = TRUE)

# 2. Bind all files together ----

synthetic_data <- map_dfr(files_list, ~read_csv(.))

# 3. Check incorrect variable names ----

setdiff(colnames(synthetic_data), c(var_names, "organismID"))

# 4. Remove useless data sets and variables ----

rm(files_list)

```

# Taxonomical re-categorisation

```{r}

# 1. Misc. corrections on organismID levels ----

synthetic_data <- synthetic_data %>% 
  # Homogenize organismID
  mutate(organismID = str_to_lower(organismID), # To lowercase
         organismID = gsub("[()]", "", organismID), # Remove parentheses
         organismID = iconv(organismID, from = 'UTF-8', to = 'ASCII//TRANSLIT'), # Convert accent letters
         organismID = str_replace_all(organismID, c("&" = "and",
                                          "_" = " ",
                                          "," = " ")), # Convert some special characters
         organismID = str_squish(organismID), 
         organismID = str_trim(organismID)) # Remove duplicated space and space before and after

```

```{r}

# 1. Find upper tax. levels and manually complete those not found ----

# 1.A If older file exist --

if(file.exists("./../data/03_tax-recategorisation.csv")){
  
  # 1.A.1 Import it -
  
  current_unique_organismID <- synthetic_data %>% select(organismID) %>% distinct() %>% pull() # vector

  old_unique_organismID <- read.csv2("./../data/03_tax-recategorisation.csv") %>% 
    filter(organismID %in% current_unique_organismID) # Remove organismID that have been deleted
  
  current_unique_organismID <- synthetic_data %>% select(organismID) %>% distinct() # tibble
  
  # 1.A.2 Find rows absent in the older file (old_unique_organismID) but present 
  # in the current one (current_unique_organismID) and complete it -
  
  missing_organismID <- anti_join(current_unique_organismID, old_unique_organismID, by = c("organismID")) %>% 
    pull(.) %>% 
    tax_name(sci = ., 
             get = c("phylum", "class", "subclass", "order", "family", "genus", "species"), 
             db = "ncbi",
             messages = FALSE,
             ask = FALSE) # Disable choice if "more than one UID found"

  if(nrow(missing_organismID) > 0){
    
      missing_organismID %>%
        select(-db) %>% 
        rename(organismID = query) %>% 
        mutate(category = NA, subcategory = NA, condition = NA, .after = "organismID") %>% 
        mutate(checked = FALSE) %>% 
        bind_rows(old_unique_organismID, .) %>%
        arrange(organismID) %>%  
        write.csv2(., file = "./../data/03_tax-recategorisation.csv", row.names = FALSE)
    
  }
  
  rm(current_unique_organismID, old_unique_organismID, missing_organismID)

}else{
  
  # 1.B Else export it --
  
  synthetic_data %>% 
    select(organismID) %>% 
    distinct() %>% 
    pull(.) %>% 
    tax_name(sci = ., 
             get = c("phylum", "class", "subclass", "order", "family", "genus", "species"), 
             db = "ncbi",
             messages = FALSE, 
             ask = FALSE) %>% # Disable choice if "more than one UID found"
    select(-db) %>% 
    rename(organismID = query) %>% 
    mutate(category = NA, subcategory = NA, condition = NA, .after = "organismID") %>% 
    mutate(checked = FALSE) %>% 
    arrange(organismID) %>% 
    write.csv2(., file = "./../data/03_tax-recategorisation.csv", row.names = FALSE)
  
}

# ----------------------------------------------------------------------------------------- #
# /!\          Before to run the next chunk code, fill the exported csv file:           /!\ #
# /!\          Manually complete empty tax. variables for each unique organismID             /!\ #
# /!\  The text of the column "organismID" must not be changed: it's the grouping variable   /!\ #
# ----------------------------------------------------------------------------------------- #

```

# Join taxonomy

```{r}

# 1. Add "category" and "subcategory" ----

unique_organismID <- read.csv2("./../data/03_tax-recategorisation.csv") %>% 
  # Remove eventual white spaces
  mutate_at(c("category", "subcategory", "condition", "phylum", "class", "subclass", "order", 
              "family", "genus", "species"), ~str_squish(str_trim(., side = "both"))) %>% 
  # Assign "subcategory"
  mutate(subcategory = case_when(phylum == "Cyanobacteria" ~ "Cyanobacteria",
                                 order == "Corallinales" ~ "Coralline algae",
                                 TRUE ~ subcategory)) %>% 
  # Assign "category"
  mutate(category = case_when(order == "Scleractinia" ~ "Hard coral",
                              phylum %in% c("Porifera", "Chordata", "Echinodermata") ~ "Other fauna",
                              phylum == "Cyanobacteria" ~ "Algae",
                              class %in% c("Ascidiacea", "Hydrozoa", "Crinoidea", 
                                           "Bivalvia", "Echinoidea", "Anthozoa") ~ "Other fauna",
                              order %in% c("Actiniaria", "Alcyonacea", "Zoantharia", 
                                           "Corallimorpharia", "Antipatharia") ~ "Other fauna",
                              subclass %in% c("Octocorallia") ~ "Other fauna",
                              phylum %in% c("Annelida") ~ "Other fauna",
                              family %in% c("Milleporidae", "Helioporidae") ~ "Hard coral",
                              class %in% c("Ulvophyceae", "Florideophyceae", "Phaeophyceae") ~ "Algae",
                              phylum %in% c("Chlorophyta", "Rhodophyta", "Ochrophyta") ~ "Algae",
                              TRUE ~ category)) %>% 
  select(-checked)

# 2. Join main data and recategorized organismID ----

synthetic_data <- left_join(synthetic_data, unique_organismID, by = "organismID")

# 3. Check if 'genus' is correct (i.e. if it correspond to the first word of 'species') ----

genus_check <- unique_organismID %>%
  filter(!is.na(species))

if(all(str_split_fixed(genus_check$species, " ", 2)[,1] == genus_check$genus, na.rm = TRUE) == FALSE){
  stop("All genus names contained in the 'Species' variable are NOT identical to those in the 'Genus' variable")
}

# 4. Control the filled categories ----

synthetic_data %>% 
  drop_na(category) %>% 
  select(organismID, category, subcategory, condition, phylum, 
         class, subclass, order, family, genus, species) %>% 
  distinct(.) %>% 
  formattable(.) %>% 
  as.datatable(., rownames = FALSE)

# 5. Control the unfilled categories (which will be removed) ----

synthetic_data %>% 
  filter(is.na(category)) %>% 
  select(organismID, category, subcategory, condition, phylum,
         class, subclass, order, family, genus, species) %>% 
  distinct(.) %>% 
  formattable(.) %>% 
  as.datatable(., rownames = FALSE)

# 6. Remove useless data sets and variables ----

rm(genus_check, unique_organismID)

```

```{r}

# 7. Make the sum of percentage cover for identical levels ----

synthetic_data <- synthetic_data %>% 
  group_by(across(c(-measurementValue))) %>% 
  summarise(measurementValue = sum(measurementValue)) %>% 
  ungroup()

# 8. Remove rows ----

synthetic_data <- synthetic_data %>% 
  filter(!is.na(category), # Remove rows containing NA for 'category'
         !is.na(measurementValue)) # Remove rows containing NA for 'cover'

# 9. Duplicate the synthetic_data to compare removed rows ----

synthetic_data_before <- synthetic_data

```

# Spatial assignation

```{r}

# 1. Extract site coordinates and transform to sf format ----

synthetic_data_coords <- synthetic_data %>% 
  drop_na(decimalLatitude, decimalLongitude) %>%
  select(datasetID, decimalLatitude, decimalLongitude) %>% 
  distinct() %>% 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), crs = 4326)

```

## GCRMN region

```{r}

# 1. Load data ----

load("../data/06_gcrmn-regions.RData")

# 2. Make the spatial join ----

synthetic_data <- st_join(synthetic_data_coords, data_gcrmn_regions) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, higherGeography = gcrmn_region) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

```

## EEZ

```{r}

# 1. Load data ----

data_eez <- st_read("../data/07_data-eez/eez_v11.shp") %>% 
  select(SOVEREIGN1, TERRITORY1) %>% 
  st_transform(crs = 4326) %>% 
  st_make_valid()

# 2. Remove holes in EEZ polygons ----

data_eez <- nngeo::st_remove_holes(data_eez)

# 3. Make the spatial join ----

synthetic_data <- st_join(synthetic_data_coords, data_eez) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X, 
         country = SOVEREIGN1, territory = TERRITORY1) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 6. Remove useless data sets and variables ----

rm(data_gcrmn_regions, data_eez)

```

# Re-order variables

```{r}

synthetic_data <- synthetic_data %>% 
  rename(scientificName = species) %>% 
  select(datasetID, higherGeography, country, territory, locality, habitat, parentEventID,
         eventID, decimalLatitude, decimalLongitude, verbatimDepth, year, month, day, 
         eventDate, samplingProtocol, recordedBy, category, subcategory, condition, 
         phylum, class, order, family, genus, scientificName, measurementValue)

```

# Quality checks

```{r}

# 1. Quality checks number 1, 2, 3, 6, 7, and 9 ----

synthetic_data <- synthetic_data %>% 
  mutate(qc1 = if_else(!(is.na(decimalLatitude)) & !(is.na(decimalLongitude)), TRUE, FALSE),
         qc2 = if_else(decimalLatitude >= -90 & decimalLatitude <= 90, TRUE, FALSE),
         qc3 = if_else(decimalLongitude >= -180 & decimalLongitude <= 180, TRUE, FALSE),
         qc6 = if_else(verbatimDepth >= 0 & verbatimDepth < 100, TRUE, FALSE),
         qc7 = if_else(!(is.na(year)), TRUE, FALSE),
         qc9 = if_else(measurementValue >= 0 & measurementValue <= 100, TRUE, FALSE))

# 2. Quality check number 4 ----

reef_buffer <- st_read("../data/08_quality-checks-buffer/buffer-reef.shp")

synthetic_data <- st_intersects(synthetic_data_coords, reef_buffer, sparse = FALSE) %>% 
  as_tibble() %>% 
  rename(qc4 = 1) %>% 
  bind_cols(synthetic_data_coords, .) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 3. Quality check number 5 ----

land_buffer <- st_read("../data/08_quality-checks-buffer/buffer-land.shp") %>% 
  st_transform(crs = 4326) %>% 
  st_wrap_dateline(options = "WRAPDATELINE=YES") %>% 
  st_make_valid()

synthetic_data <- st_intersects(synthetic_data_coords, land_buffer, sparse = FALSE) %>% 
  as_tibble() %>% 
  mutate(qc5 = if_any(.cols = contains('V'), ~ isTRUE(.))) %>% 
  select(qc5) %>% 
  mutate(qc5 = !qc5) %>% # Reverse False/True statement 
  bind_cols(synthetic_data_coords, .) %>% 
  bind_cols(., st_coordinates(.)) %>% 
  rename(decimalLatitude = Y, decimalLongitude = X) %>% 
  st_drop_geometry() %>% 
  left_join(synthetic_data, .)

# 4. Quality check number 8 ----

synthetic_data <- synthetic_data %>% 
  group_by(datasetID, locality, habitat, parentEventID,
         eventID, decimalLatitude, decimalLongitude, verbatimDepth, 
         year, month, day, eventDate) %>% 
  summarise(qc8 = sum(measurementValue)) %>% 
  ungroup() %>% 
  mutate(qc8 = if_else(qc8 >= 0 & qc8 <= 100, TRUE, FALSE)) %>% 
  left_join(synthetic_data, .)

# 5. Combine and transform data ----

quality_checks <- synthetic_data %>% 
  select(datasetID, starts_with("qc")) %>% 
  pivot_longer(2:ncol(.), values_to = "type", names_to = "QC") %>% 
  group_by(datasetID, QC, type) %>% 
  count() %>% 
  ungroup() %>% 
  group_by(datasetID, QC) %>% 
  mutate(percent = (n*100)/sum(n)) %>% 
  ungroup() %>% 
  mutate(question = case_when(QC == "qc1" ~ "Are the latitude and longitude available?",
                              QC == "qc2" ~ "Is the latitude within its possible boundaries?",
                              QC == "qc3" ~ "Is the longitude within its possible boundaries?",
                              QC == "qc4" ~ "Is the site within the coral reef distribution area?",
                              QC == "qc5" ~ "Is the site located off lands?",
                              QC == "qc6" ~ "Is the depth between 0 and 100 meters?",
                              QC == "qc7" ~ "Is the year available?",
                              QC == "qc8" ~ "Is the sum of the percentage cover of benthic categories within\
                              the sampling unit greater than 0 and lower than 100?",
                              QC == "qc9" ~ "Is the percentage cover of a given benthic category\
                              greater than 0 and lower than 100?"),
         QC = str_replace_all(QC, "qc", "Quality check #"))

# 6. Remove useless data sets and variables ----

rm(reef_buffer, land_buffer, synthetic_data_coords)

```

# Individual summary

```{r}

map(unique(synthetic_data$datasetID), ~render_rmd(.))

```
















### Metric variable

```{r}

# 1. First check of the total cover ----

# 1.1 Make the sum of percentage cover by sampling unit --

total_cover <- synthetic_data %>% 
  group_by(dataset_id, location, site, zone, quadrat,
           lat, long, year, date, replicate, observer, depth) %>% 
  summarise(total = sum(cover),
            total = round(total, 3)) # Round total to remove impact of small digits on filtering

# 1.2 Table of number of sampling units in the different categories, by dataset_id --

total_cover %>% 
  select(dataset_id, total) %>% 
  group_by(dataset_id) %>% 
  summarise(Less_0 = length(which(total < 0)), 
            Between_0_100 = length(which(total > 0 & total < 100)), 
            Equal_100 = length(which(total == 100)), 
            More_100 = length(which(total > 100)),
            Min = round(min(total, na.rm = TRUE), 2),
            Max = round(max(total, na.rm = TRUE), 2)) %>% 
  datatable(., rownames = FALSE,
            colnames = c("Dataset", "Cover < 0", 
                         "0 < Cover < 100", 
                         "Cover = 100", 
                         "Cover > 100", "Min", "Max")) %>% 
  formatStyle("Less_0", backgroundColor = "#e74c3c") %>% 
  formatStyle("Between_0_100", backgroundColor = "#f4b350") %>% 
  formatStyle("Equal_100", backgroundColor = "#16a085") %>% 
  formatStyle("More_100", backgroundColor = "#e74c3c")

# 1.3 Join the total cover with main data --

synthetic_data <- synthetic_data %>%
  left_join(., total_cover)

# 2. Adjust the cover (using the rule of three) ----

treshold <- 101 # Define the threshold

synthetic_data <- synthetic_data %>% 
  mutate(cover = ifelse(total > 100 & total < treshold, (cover*100)/total, cover))

# 3. First check of the total cover ----

# 3.1 Make the sum of percentage cover by sampling unit --

total_cover <- synthetic_data %>% 
  group_by(dataset_id, location, site, zone, quadrat,
           lat, long, year, date, replicate, observer, depth) %>% 
  summarise(total = sum(cover),
            total = round(total, 3)) # Round total to remove impact of small digits on filtering

# 3.2 Table of number of sampling units in the different categories, by dataset_id --

total_cover %>% 
  select(dataset_id, total) %>% 
  group_by(dataset_id) %>% 
  summarise(Less_0 = length(which(total < 0)), 
            Between_0_100 = length(which(total > 0 & total < 100)), 
            Equal_100 = length(which(total == 100)), 
            More_100 = length(which(total > 100)),
            Min = round(min(total, na.rm = TRUE), 2),
            Max = round(max(total, na.rm = TRUE), 2)) %>% 
  datatable(., rownames = FALSE,
            colnames = c("Dataset", "Cover < 0", 
                         "0 < Cover < 100", 
                         "Cover = 100", 
                         "Cover > 100", "Min", "Max")) %>% 
  formatStyle("Less_0", backgroundColor = "#e74c3c") %>% 
  formatStyle("Between_0_100", backgroundColor = "#f4b350") %>% 
  formatStyle("Equal_100", backgroundColor = "#16a085") %>% 
  formatStyle("More_100", backgroundColor = "#e74c3c")

# 3.3 Join the total cover with main data --

synthetic_data <- synthetic_data %>%
  select(-total) %>% 
  left_join(., total_cover)

```

## Remove errors

```{r}

# 1. Remove rows with errors ----

synthetic_data <- synthetic_data %>% 
  filter(total > 0 & total <= 100) %>% 
  filter(cover > 0 & cover <= 100) %>% 
  drop_na(lat, long)

# 2. Number and percentage of rows removed by dataset_id ----

left_join(synthetic_data_before %>% group_by(dataset_id) %>% count(name = "n_before"),
          synthetic_data %>% group_by(dataset_id) %>% count(name = "n_after")) %>% 
  mutate(n_removed_abs = n_before - n_after,
         n_removed_rel = (n_removed_abs/n_before)*100) %>% 
  formattable(., list(Removed = color_bar("#e74c3c"))) %>% 
  as.datatable(., rownames = FALSE, colnames = c("dataset_id", "n rows before", 
                                                 "n rows after", "n rows removed", 
                                                 "n rows removed (%)"))

```

# Export data

```{r}

# 1. Export the data ----

save(synthetic_data, file = "./../data/04_benthic-cover_synthetic-dataset.RData")

```

# Reproducibility

```{r reprod}

# 1. Reproducibility ----

sessionInfo()

```

---
Jeremy WICQUART | jeremywicquart@gmail.com | `r format(Sys.time())`